{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_RvUjMTHzfzZ",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Process-Data\" data-toc-modified-id=\"Process-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Process Data</a></span></li><li><span><a href=\"#Pre-Process-Data-For-Deep-Learning\" data-toc-modified-id=\"Pre-Process-Data-For-Deep-Learning-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Pre-Process Data For Deep Learning</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Look-at-one-example-of-processed-issue-bodies\" data-toc-modified-id=\"Look-at-one-example-of-processed-issue-bodies-2.0.0.1\"><span class=\"toc-item-num\">2.0.0.1&nbsp;&nbsp;</span>Look at one example of processed issue bodies</a></span></li><li><span><a href=\"#Look-at-one-example-of-processed-issue-titles\" data-toc-modified-id=\"Look-at-one-example-of-processed-issue-titles-2.0.0.2\"><span class=\"toc-item-num\">2.0.0.2&nbsp;&nbsp;</span>Look at one example of processed issue titles</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Define-Model-Architecture\" data-toc-modified-id=\"Define-Model-Architecture-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Define Model Architecture</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Load-the-data-from-disk-into-variables\" data-toc-modified-id=\"Load-the-data-from-disk-into-variables-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Load the data from disk into variables</a></span></li><li><span><a href=\"#Define-Model-Architecture\" data-toc-modified-id=\"Define-Model-Architecture-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Define Model Architecture</a></span></li></ul></li></ul></li><li><span><a href=\"#Train-Model\" data-toc-modified-id=\"Train-Model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Train Model</a></span></li><li><span><a href=\"#See-Results-On-Holdout-Set\" data-toc-modified-id=\"See-Results-On-Holdout-Set-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>See Results On Holdout Set</a></span></li><li><span><a href=\"#Feature-Extraction-Demo\" data-toc-modified-id=\"Feature-Extraction-Demo-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Feature Extraction Demo</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Example-1:-Issues-Installing-Python-Packages\" data-toc-modified-id=\"Example-1:-Issues-Installing-Python-Packages-6.0.1\"><span class=\"toc-item-num\">6.0.1&nbsp;&nbsp;</span>Example 1: Issues Installing Python Packages</a></span></li><li><span><a href=\"#Example-2:--Issues-asking-for-feature-improvements\" data-toc-modified-id=\"Example-2:--Issues-asking-for-feature-improvements-6.0.2\"><span class=\"toc-item-num\">6.0.2&nbsp;&nbsp;</span>Example 2:  Issues asking for feature improvements</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GCE_PROJECT_NAME='My First Project'\n",
      "env: TPU_ZONE=us-central1-f\n",
      "env: TPU_NAME=digitransolutions1\n",
      "env: TPU_IP_RANGE=10.240.1.0/29\n",
      "env: GCS_DATA_PATH=gs://digitran-data-bucket/mnist/\n",
      "env: GCS_CKPT_PATH=gs://digitran-checkpoint-bucket/mnist/\n"
     ]
    }
   ],
   "source": [
    "%env GCE_PROJECT_NAME 'My First Project'\n",
    "%env TPU_ZONE us-central1-f\n",
    "%env TPU_NAME digitransolutions1\n",
    "%env TPU_IP_RANGE 10.240.1.0/29\n",
    "%env GCS_DATA_PATH gs://digitran-data-bucket/mnist/\n",
    "%env GCS_CKPT_PATH gs://digitran-checkpoint-bucket/mnist/\n",
    "    \n",
    "# Automatically get bucket name from GCS paths\n",
    "import os\n",
    "os.environ['GCS_DATA_BUCKET'] = os.environ['GCS_DATA_PATH'][5:].split('/')[0]\n",
    "os.environ['GCS_CKPT_BUCKET'] = os.environ['GCS_CKPT_PATH'][5:].split('/')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [compute/zone].\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.alpha.compute.tpus.create) ALREADY_EXISTS: Resource 'projects/modern-impulse-280709/locations/us-central1-f/nodes/digitransolutions1' already exists\n",
      "- '@type': type.googleapis.com/google.rpc.ResourceInfo\n",
      "  resourceName: projects/modern-impulse-280709/locations/us-central1-f/nodes/digitransolutions1\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set compute/zone $TPU_ZONE\n",
    "!gcloud alpha compute tpus create $TPU_NAME --range=$TPU_IP_RANGE --accelerator-type=tpu-v2 --version=nightly --zone=$TPU_ZONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://digitran-data-bucket/...\n",
      "ServiceException: 409 Bucket digitran-data-bucket already exists.\n",
      "Creating gs://digitran-checkpoint-bucket/...\n",
      "ServiceException: 409 Bucket digitran-checkpoint-bucket already exists.\n",
      "No changes made to gs://digitran-data-bucket/\n",
      "No changes made to gs://digitran-checkpoint-bucket/\n",
      "Successfully set permissions!\n"
     ]
    }
   ],
   "source": [
    "!gsutil mb -c regional -l us-central1 gs://$GCS_DATA_BUCKET\n",
    "!gsutil mb -c regional -l us-central1 gs://$GCS_CKPT_BUCKET\n",
    "\n",
    "!gsutil iam ch serviceAccount:`gcloud alpha compute tpus describe $TPU_NAME | grep serviceAccount | cut -d' ' -f2`:admin gs://$GCS_DATA_BUCKET gs://$GCS_CKPT_BUCKET && echo 'Successfully set permissions!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'gs://digitran-data-bucket/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 788
    },
    "colab_type": "code",
    "id": "TKP0Dvpazfzd",
    "outputId": "6959b285-a38f-43b9-9fb7-e0cfd67a37ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: digitransolutions1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: digitransolutions1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  ['10.240.1.2:8470']\n",
      "Number of accelerators:  8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Detect hardware\n",
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
    "except ValueError: # If TPU not found\n",
    "  tpu = None\n",
    "# Select appropriate distribution strategy\n",
    "if tpu:\n",
    "  tf.config.experimental_connect_to_cluster(tpu)\n",
    "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])  \n",
    "else:\n",
    "  strategy = tf.distribute.get_strategy() # Default strategy that works on CPU and single GPU\n",
    "  print('Running on CPU instead')\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-24 22:35:47--  https://docs.google.com/uc?export=download&confirm=dDnn&id=1EzcbYa4aRvLcTCn6xhlP1iG8fL8baNsb\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.212.102, 172.217.212.100, 172.217.212.101, ...\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.212.102|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0k-8k-docs.googleusercontent.com/docs/securesc/gbb1teob90hbs7efjfek4aaer8c6b9cu/jsgf1dl3ailrd7q9l9aps94opao62ua1/1593038100000/09218841784108975906/08623641984017747873Z/1EzcbYa4aRvLcTCn6xhlP1iG8fL8baNsb?e=download [following]\n",
      "--2020-06-24 22:35:47--  https://doc-0k-8k-docs.googleusercontent.com/docs/securesc/gbb1teob90hbs7efjfek4aaer8c6b9cu/jsgf1dl3ailrd7q9l9aps94opao62ua1/1593038100000/09218841784108975906/08623641984017747873Z/1EzcbYa4aRvLcTCn6xhlP1iG8fL8baNsb?e=download\n",
      "Resolving doc-0k-8k-docs.googleusercontent.com (doc-0k-8k-docs.googleusercontent.com)... 173.194.197.132, 2607:f8b0:4001:c1b::84\n",
      "Connecting to doc-0k-8k-docs.googleusercontent.com (doc-0k-8k-docs.googleusercontent.com)|173.194.197.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://docs.google.com/nonceSigner?nonce=69g2sm9dif2k2&continue=https://doc-0k-8k-docs.googleusercontent.com/docs/securesc/gbb1teob90hbs7efjfek4aaer8c6b9cu/jsgf1dl3ailrd7q9l9aps94opao62ua1/1593038100000/09218841784108975906/08623641984017747873Z/1EzcbYa4aRvLcTCn6xhlP1iG8fL8baNsb?e%3Ddownload&hash=co1fhipojjqq3vt6c5iqok7ns01u9fcf [following]\n",
      "--2020-06-24 22:35:47--  https://docs.google.com/nonceSigner?nonce=69g2sm9dif2k2&continue=https://doc-0k-8k-docs.googleusercontent.com/docs/securesc/gbb1teob90hbs7efjfek4aaer8c6b9cu/jsgf1dl3ailrd7q9l9aps94opao62ua1/1593038100000/09218841784108975906/08623641984017747873Z/1EzcbYa4aRvLcTCn6xhlP1iG8fL8baNsb?e%3Ddownload&hash=co1fhipojjqq3vt6c5iqok7ns01u9fcf\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.212.102|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://doc-0k-8k-docs.googleusercontent.com/docs/securesc/gbb1teob90hbs7efjfek4aaer8c6b9cu/jsgf1dl3ailrd7q9l9aps94opao62ua1/1593038100000/09218841784108975906/08623641984017747873Z/1EzcbYa4aRvLcTCn6xhlP1iG8fL8baNsb?e=download&nonce=69g2sm9dif2k2&user=08623641984017747873Z&hash=qut86i5410rvktc4uja37g72su1v16q0 [following]\n",
      "--2020-06-24 22:35:47--  https://doc-0k-8k-docs.googleusercontent.com/docs/securesc/gbb1teob90hbs7efjfek4aaer8c6b9cu/jsgf1dl3ailrd7q9l9aps94opao62ua1/1593038100000/09218841784108975906/08623641984017747873Z/1EzcbYa4aRvLcTCn6xhlP1iG8fL8baNsb?e=download&nonce=69g2sm9dif2k2&user=08623641984017747873Z&hash=qut86i5410rvktc4uja37g72su1v16q0\n",
      "Connecting to doc-0k-8k-docs.googleusercontent.com (doc-0k-8k-docs.googleusercontent.com)|173.194.197.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/csv]\n",
      "Saving to: ‘github_issues.csv’\n",
      "\n",
      "github_issues.csv       [ <=>                ]   2.66G  85.4MB/s    in 31s     \n",
      "\n",
      "2020-06-24 22:36:19 (86.9 MB/s) - ‘github_issues.csv’ saved [2852401417]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1EzcbYa4aRvLcTCn6xhlP1iG8fL8baNsb' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1EzcbYa4aRvLcTCn6xhlP1iG8fL8baNsb\" -O github_issues.csv && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "FyuMSLWZ08Rf",
    "outputId": "45c0d87f-44f2-4bbc-c6a2-906754946e7d"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-453322b83b27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/gdrive/My Drive/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'seq2seq-model/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "root_dir = \"/content/gdrive/My Drive/\"\n",
    "base_dir = root_dir + 'seq2seq-model/'\n",
    "import sys\n",
    "sys.path.append(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "rvYAFKU-8wS_",
    "outputId": "928a5815-2db2-47e9-abb9-c80a99c7abfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wget in /home/digitransolutions1/.local/lib/python3.7/site-packages (3.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nmslib in /home/digitransolutions1/.local/lib/python3.7/site-packages (2.0.6)\n",
      "Requirement already satisfied: pybind11>=2.2.3 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from nmslib) (2.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from nmslib) (1.18.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from nmslib) (5.7.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pathos in /home/digitransolutions1/.local/lib/python3.7/site-packages (0.2.6)\n",
      "Requirement already satisfied: dill>=0.3.2 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from pathos) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.10 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from pathos) (0.70.10)\n",
      "Requirement already satisfied: ppft>=1.6.6.2 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from pathos) (1.6.6.2)\n",
      "Requirement already satisfied: pox>=0.2.8 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from pathos) (0.2.8)\n",
      "Requirement already satisfied: six>=1.7.3 in /usr/local/lib/python3.7/dist-packages (from ppft>=1.6.6.2->pathos) (1.14.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/digitransolutions1/.local/lib/python3.7/site-packages (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: more_itertools in /home/digitransolutions1/.local/lib/python3.7/site-packages (8.4.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/digitransolutions1/.local/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (46.0.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy) (7.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.18.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.43.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: en_core_web_sm==2.3.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.0/en_core_web_sm-2.3.0.tar.gz#egg=en_core_web_sm==2.3.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from en_core_web_sm==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2.0.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.1.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (4.43.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (7.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.18.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (46.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2.23.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (0.7.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.5.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.0) (3.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/digitransolutions1/.local/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "/home/digitransolutions1/.local/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ktext in /home/digitransolutions1/.local/lib/python3.7/site-packages (0.40)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ktext) (1.18.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from ktext) (1.4.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from ktext) (1.14.0)\n",
      "Requirement already satisfied: more-itertools in /home/digitransolutions1/.local/lib/python3.7/site-packages (from ktext) (8.4.0)\n",
      "Requirement already satisfied: pyarrow in /home/digitransolutions1/.local/lib/python3.7/site-packages (from ktext) (0.17.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ktext) (5.3)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from ktext) (2.1.0)\n",
      "Requirement already satisfied: pandas>=0.21.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from ktext) (1.0.5)\n",
      "Requirement already satisfied: pathos in /home/digitransolutions1/.local/lib/python3.7/site-packages (from ktext) (0.2.6)\n",
      "Requirement already satisfied: keras==2.2.4 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from ktext) (2.2.4)\n",
      "Requirement already satisfied: msgpack-numpy in /home/digitransolutions1/.local/lib/python3.7/site-packages (from ktext) (0.4.6.post0)\n",
      "Requirement already satisfied: dask in /home/digitransolutions1/.local/lib/python3.7/site-packages (from ktext) (2.19.0)\n",
      "Requirement already satisfied: msgpack in /home/digitransolutions1/.local/lib/python3.7/site-packages (from ktext) (1.0.0)\n",
      "Requirement already satisfied: textacy==0.6.2 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from ktext) (0.6.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->ktext) (1.27.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->ktext) (1.12.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->ktext) (3.11.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->ktext) (0.2.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->ktext) (0.2.2)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->ktext) (2.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->ktext) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->ktext) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->ktext) (2.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->ktext) (1.0.8)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->ktext) (3.2.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->ktext) (0.8.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->ktext) (0.9.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow->ktext) (0.34.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->ktext) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->ktext) (2019.3)\n",
      "Requirement already satisfied: dill>=0.3.2 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from pathos->ktext) (0.3.2)\n",
      "Requirement already satisfied: ppft>=1.6.6.2 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from pathos->ktext) (1.6.6.2)\n",
      "Requirement already satisfied: pox>=0.2.8 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from pathos->ktext) (0.2.8)\n",
      "Requirement already satisfied: multiprocess>=0.70.10 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from pathos->ktext) (0.70.10)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4->ktext) (2.10.0)\n",
      "Requirement already satisfied: tqdm>=4.11.1 in /usr/local/lib/python3.7/dist-packages (from textacy==0.6.2->ktext) (4.43.0)\n",
      "Requirement already satisfied: networkx>=1.11 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from textacy==0.6.2->ktext) (2.4)\n",
      "Requirement already satisfied: spacy>=2.0.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from textacy==0.6.2->ktext) (2.3.0)\n",
      "Requirement already satisfied: ftfy<5.0.0,>=4.2.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from textacy==0.6.2->ktext) (4.4.3)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from textacy==0.6.2->ktext) (4.0.0)\n",
      "Requirement already satisfied: ijson>=2.3 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from textacy==0.6.2->ktext) (3.0.4)\n",
      "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from textacy==0.6.2->ktext) (2.23.0)\n",
      "Requirement already satisfied: pyemd>=0.3.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from textacy==0.6.2->ktext) (0.5.1)\n",
      "Requirement already satisfied: scikit-learn>=0.17.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from textacy==0.6.2->ktext) (0.20.2)\n",
      "Requirement already satisfied: pyphen>=0.9.4 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from textacy==0.6.2->ktext) (0.9.5)\n",
      "Requirement already satisfied: cytoolz>=0.8.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from textacy==0.6.2->ktext) (0.10.1)\n",
      "Requirement already satisfied: unidecode>=0.04.19 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from textacy==0.6.2->ktext) (1.1.1)\n",
      "Requirement already satisfied: python-levenshtein>=0.12.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from textacy==0.6.2->ktext) (0.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow->ktext) (46.0.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow->ktext) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow->ktext) (3.2.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow->ktext) (1.0.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow->ktext) (1.11.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=1.11->textacy==0.6.2->ktext) (4.4.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy>=2.0.0->textacy==0.6.2->ktext) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy>=2.0.0->textacy==0.6.2->ktext) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy>=2.0.0->textacy==0.6.2->ktext) (1.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy>=2.0.0->textacy==0.6.2->ktext) (3.0.2)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy>=2.0.0->textacy==0.6.2->ktext) (7.4.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy>=2.0.0->textacy==0.6.2->ktext) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy>=2.0.0->textacy==0.6.2->ktext) (0.7.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy>=2.0.0->textacy==0.6.2->ktext) (2.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy>=2.0.0->textacy==0.6.2->ktext) (1.0.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy<5.0.0,>=4.2.0->textacy==0.6.2->ktext) (0.1.8)\n",
      "Requirement already satisfied: html5lib in /home/digitransolutions1/.local/lib/python3.7/site-packages (from ftfy<5.0.0,>=4.2.0->textacy==0.6.2->ktext) (1.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy==0.6.2->ktext) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy==0.6.2->ktext) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy==0.6.2->ktext) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy==0.6.2->ktext) (1.25.8)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from cytoolz>=0.8.0->textacy==0.6.2->ktext) (0.10.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow->ktext) (1.3.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow->ktext) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow->ktext) (0.2.8)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.0->textacy==0.6.2->ktext) (1.5.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from html5lib->ftfy<5.0.0,>=4.2.0->textacy==0.6.2->ktext) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow->ktext) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow->ktext) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.0->textacy==0.6.2->ktext) (3.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: annoy in /home/digitransolutions1/.local/lib/python3.7/site-packages (1.16.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fastai in /home/digitransolutions1/.local/lib/python3.7/site-packages (1.0.61)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai) (5.3)\n",
      "Requirement already satisfied: pandas in /home/digitransolutions1/.local/lib/python3.7/site-packages (from fastai) (1.0.5)\n",
      "Requirement already satisfied: numexpr in /home/digitransolutions1/.local/lib/python3.7/site-packages (from fastai) (2.7.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.18.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai) (2.23.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai) (1.4.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fastai) (7.0.0)\n",
      "Requirement already satisfied: torchvision in /home/digitransolutions1/.local/lib/python3.7/site-packages (from fastai) (0.6.1)\n",
      "Requirement already satisfied: packaging in /home/digitransolutions1/.local/lib/python3.7/site-packages (from fastai) (20.4)\n",
      "Requirement already satisfied: spacy>=2.0.18; python_version < \"3.8\" in /home/digitransolutions1/.local/lib/python3.7/site-packages (from fastai) (2.3.0)\n",
      "Requirement already satisfied: torch>=1.0.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from fastai) (1.5.1)\n",
      "Requirement already satisfied: nvidia-ml-py3 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from fastai) (7.352.0)\n",
      "Requirement already satisfied: fastprogress>=0.2.1 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from fastai) (0.2.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from fastai) (4.9.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai) (3.2.0)\n",
      "Requirement already satisfied: bottleneck in /home/digitransolutions1/.local/lib/python3.7/site-packages (from fastai) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai) (2019.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (3.0.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from packaging->fastai) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->fastai) (2.4.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (3.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (4.43.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (1.1.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (0.4.1)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (7.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (0.7.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (46.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (1.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (1.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai) (2.0.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->fastai) (0.18.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from beautifulsoup4->fastai) (2.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (1.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.18; python_version < \"3.8\"->fastai) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.18; python_version < \"3.8\"->fastai) (3.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/digitransolutions1/.local/lib/python3.7/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.18.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch) (0.18.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchtext in /home/digitransolutions1/.local/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: torch in /home/digitransolutions1/.local/lib/python3.7/site-packages (from torchtext) (1.5.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.14.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.43.0)\n",
      "Requirement already satisfied: sentencepiece in /home/digitransolutions1/.local/lib/python3.7/site-packages (from torchtext) (0.1.91)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.18.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch->torchtext) (0.18.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.9)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchvision in /home/digitransolutions1/.local/lib/python3.7/site-packages (0.6.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.0.0)\n",
      "Requirement already satisfied: torch==1.5.1 in /home/digitransolutions1/.local/lib/python3.7/site-packages (from torchvision) (1.5.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.18.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.1->torchvision) (0.18.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn==0.20.2 in /home/digitransolutions1/.local/lib/python3.7/site-packages (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.20.2) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.20.2) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install astor\n",
    "!pip install wget\n",
    "!pip install nmslib\n",
    "!pip install pathos\n",
    "!pip install pandas\n",
    "!pip install more_itertools\n",
    "!pip install spacy\n",
    "!python3 -m spacy download en\n",
    "!pip install ktext\n",
    "!pip install annoy\n",
    "!pip install fastai\n",
    "!pip install torch\n",
    "!pip install torchtext\n",
    "!pip install torchvision\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "!pip install scikit-learn==0.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VSLi5mEd0oR5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "wgg4djW__Nqv",
    "outputId": "fe2b114d-8514-4b3d-8e24-df63fed352e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_FILE =  data/github_issues.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# INPUT_PATH = Path(base_dir)\n",
    "base_dir = 'data/'\n",
    "INPUT_PATH = Path(base_dir)\n",
    "INPUT_FILE = Path(INPUT_PATH/'github_issues.csv')\n",
    "print(\"INPUT_FILE = \", INPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BdTewsxjzf0A"
   },
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PxfMUyISzf0F"
   },
   "source": [
    "Look at filesystem to see files extracted from BigQuery (or Kaggle: https://www.kaggle.com/davidshinn/github-issues/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tc8CvtRDzf0I"
   },
   "outputs": [],
   "source": [
    "!ls -lah | grep INPUT_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1SO4mXMOzf0g"
   },
   "source": [
    "Split data into train and test set and preview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "colab_type": "code",
    "id": "IwmRPpn-zf0m",
    "outputId": "113a7007-483c-43a8-cad8-22f0bf25bd65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,080,000 rows 3 columns\n",
      "Test: 120,000 rows 3 columns\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope(): \n",
    "    #read in data sample 2M rows (for speed of tutorial)\n",
    "    traindf, testdf = train_test_split(pd.read_csv(INPUT_FILE).sample(n=1200000), test_size=.10)\n",
    "\n",
    "\n",
    "    #print out stats about shape of data\n",
    "    print(f'Train: {traindf.shape[0]:,} rows {traindf.shape[1]:,} columns')\n",
    "    print(f'Test: {testdf.shape[0]:,} rows {testdf.shape[1]:,} columns')\n",
    "\n",
    "    # preview data\n",
    "    traindf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1de7Djizf04"
   },
   "source": [
    "**Convert to lists in preparation for modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "02BXpruDzf08",
    "outputId": "fa121a59-7608-46ea-c1d7-26304db4b3bb"
   },
   "outputs": [],
   "source": [
    "with strategy.scope(): \n",
    "    train_body_raw = traindf.body.tolist()\n",
    "    train_title_raw = traindf.issue_title.tolist()\n",
    "    #preview output of first element\n",
    "    train_body_raw[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4IlN9ZLzf1R"
   },
   "source": [
    "# Pre-Process Data For Deep Learning\n",
    "\n",
    "See [this repo](https://github.com/hamelsmu/ktext) for documentation on the ktext package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wNR7JZwCzf1X"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from ktext.preprocess import processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "zWTAgZmlzf1r",
    "outputId": "952e1fe3-5f58-4aee-c191-04de84631fa1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:(1/2) done. 222 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 42 sec\n",
      "WARNING:root:Finished parsing 1,080,000 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 28 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 3s, sys: 6.78 s, total: 2min 10s\n",
      "Wall time: 4min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Clean, tokenize, and apply padding / truncating such that each document length = 70\n",
    "#  also, retain only the top 8,000 words in the vocabulary and set the remaining words\n",
    "#  to 1 which will become common index for rare words \n",
    "with strategy.scope(): \n",
    "    body_pp = processor(keep_n=8000, padding_maxlen=70)\n",
    "    train_body_vecs = body_pp.fit_transform(train_body_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B9J20Vjozf11"
   },
   "source": [
    "#### Look at one example of processed issue bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "0wMEpHsjzf13",
    "outputId": "9ad23354-10b6-4203-f93c-f7659abe4546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original string:\n",
      " it would be cool if the sliders volume and track timing were working. and for example, a click on the cover opened the main page \n",
      "\n",
      "after pre-processing:\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0   10   43   15 1877   21    3    1 1384    9 1074 4001\n",
      "  412  214    9   14   96    5  264   16    3 2240  995    3  182   93] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\noriginal string:\\n', train_body_raw[0], '\\n')\n",
    "print('after pre-processing:\\n', train_body_vecs[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "colab_type": "code",
    "id": "ldF4uws4zf2C",
    "outputId": "48b3c7fc-bb1f-4e3f-b569-4a5562bee082"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:(1/2) done. 38 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 7 sec\n",
      "WARNING:root:Finished parsing 1,080,000 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 8 sec\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a text processor for the titles, with some different parameters\n",
    "#  append_indicators = True appends the tokens '_start_' and '_end_' to each\n",
    "#                      document\n",
    "#  padding = 'post' means that zero padding is appended to the end of the \n",
    "#             of the document (as opposed to the default which is 'pre')\n",
    "title_pp = processor(append_indicators=True, keep_n=4500, \n",
    "                     padding_maxlen=12, padding ='post')\n",
    "\n",
    "# process the title data\n",
    "train_title_vecs = title_pp.fit_transform(train_title_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qam8ze3Uzf2O"
   },
   "source": [
    "#### Look at one example of processed issue titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fxy-7kuozf2Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original string:\n",
      " sliders feature request\n",
      "after pre-processing:\n",
      " [ 2  1 55 43  3  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print('\\noriginal string:\\n', train_title_raw[0])\n",
    "print('after pre-processing:\\n', train_title_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sbcArnsQB78-"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/data/seq2seq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1b2bf09d2f11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'./data/seq2seq/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mOUTPUT_PATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.7/pathlib.py\u001b[0m in \u001b[0;36mmkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparents\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/data/seq2seq'"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = Path(DATA_PATH)\n",
    "OUTPUT_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJgFvjLyzf2e"
   },
   "source": [
    "Serialize all of this to disk for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EsZnf8ZRzf2f"
   },
   "outputs": [],
   "source": [
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "# Save the preprocessor\n",
    "with open('seq2seq/body_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(body_pp, f)\n",
    "\n",
    "with open('seq2seq/title_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(title_pp, f)\n",
    "\n",
    "# Save the processed data\n",
    "np.save('seq2seq/train_title_vecs.npy', train_title_vecs)\n",
    "np.save('seq2seq/train_body_vecs.npy', train_body_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.1)\n",
      "Requirement already satisfied: joblib in /home/digitransolutions1/.local/lib/python3.7/site-packages (from nltk) (0.15.1)\n",
      "Collecting regex\n",
      "  Downloading regex-2020.6.8-cp37-cp37m-manylinux2010_x86_64.whl (661 kB)\n",
      "\u001b[K     |████████████████████████████████| 661 kB 9.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.43.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434673 sha256=727c0f7c410fab7c530f8555255b56a9b8dd767aa08b6577dcc42b8c8398ce59\n",
      "  Stored in directory: /home/digitransolutions1/.cache/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
      "Successfully built nltk\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.5 regex-2020.6.8\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q3Ot7Z_Yzf2s"
   },
   "source": [
    "# Define Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gL02IQM5zf2u"
   },
   "source": [
    "### Load the data from disk into variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pyu19SJlzf2x"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "msTDN8yuzf29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (1080000, 70)\n",
      "Shape of decoder input: (1080000, 11)\n",
      "Shape of decoder target: (1080000, 11)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data, doc_length = load_encoder_inputs('seq2seq/train_body_vecs.npy')\n",
    "decoder_input_data, decoder_target_data = load_decoder_inputs('seq2seq/train_title_vecs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LND5WoNuzf3I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary for seq2seq/body_pp.dpkl: 8,002\n",
      "Size of vocabulary for seq2seq/title_pp.dpkl: 4,502\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens, body_pp = load_text_processor('seq2seq/body_pp.dpkl')\n",
    "num_decoder_tokens, title_pp = load_text_processor('seq2seq/title_pp.dpkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0E45s1C-zf3Z"
   },
   "source": [
    "### Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uwufZr2-zf30"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from seq2seq_utils import viz_model_architecture\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "with strategy.scope(): \n",
    "  #arbitrarly set latent dimension for embedding and hidden units\n",
    "  latent_dim = 300\n",
    "\n",
    "  ##### Define Model Architecture ######\n",
    "\n",
    "  ########################\n",
    "  #### Encoder Model ####\n",
    "  encoder_inputs = Input(shape=(doc_length,), name='Encoder-Input')\n",
    "\n",
    "  # Word embeding for encoder (ex: Issue Body)\n",
    "  x = Embedding(num_encoder_tokens, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
    "  x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
    "\n",
    "  # Intermediate GRU layer (optional)\n",
    "  #x = GRU(latent_dim, name='Encoder-Intermediate-GRU', return_sequences=True)(x)\n",
    "  #x = BatchNormalization(name='Encoder-Batchnorm-2')(x)\n",
    "\n",
    "  # We do not need the `encoder_output` just the hidden state.\n",
    "  _, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
    "\n",
    "  # Encapsulate the encoder as a separate entity so we can just \n",
    "  #  encode without decoding if we want to.\n",
    "  encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "\n",
    "  seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
    "\n",
    "  ########################\n",
    "  #### Decoder Model ####\n",
    "  decoder_inputs = Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
    "\n",
    "  # Word Embedding For Decoder (ex: Issue Titles)\n",
    "  dec_emb = Embedding(num_decoder_tokens, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
    "  dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "  # Set up the decoder, using `decoder_state_input` as initial state.\n",
    "  decoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
    "  decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
    "  x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
    "\n",
    "  # Dense layer for prediction\n",
    "  decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
    "  decoder_outputs = decoder_dense(x)\n",
    "\n",
    "  ########################\n",
    "  #### Seq2Seq Model ####\n",
    "\n",
    "  #seq2seq_decoder_out = decoder_model([decoder_inputs, seq2seq_encoder_out])\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDF8mLZ6U5I3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 300)    1350600     Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      [(None, 70)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 300)    1200        Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 300)          2943600     Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 300),  541800      Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 300)    1200        Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 4502)   1355102     Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 6,193,502\n",
      "Trainable params: 6,191,702\n",
      "Non-trainable params: 1,800\n",
      "__________________________________________________________________________________________________\n",
      "Train on 950400 samples, validate on 129600 samples\n",
      "Epoch 1/7\n",
      "950400/950400 [==============================] - 69s 72us/sample - loss: 2.8825 - val_loss: 2.5004\n",
      "Epoch 2/7\n",
      "950400/950400 [==============================] - 57s 60us/sample - loss: 2.3882 - val_loss: 2.3977\n",
      "Epoch 3/7\n",
      "950400/950400 [==============================] - 57s 60us/sample - loss: 2.2764 - val_loss: 2.3717\n",
      "Epoch 4/7\n",
      "950400/950400 [==============================] - 57s 60us/sample - loss: 2.2102 - val_loss: 2.3633\n",
      "Epoch 5/7\n",
      "950400/950400 [==============================] - 57s 60us/sample - loss: 2.1626 - val_loss: 2.3629\n",
      "Epoch 6/7\n",
      "950400/950400 [==============================] - 56s 59us/sample - loss: 2.1250 - val_loss: 2.3674\n",
      "Epoch 7/7\n",
      "950400/950400 [==============================] - 64s 68us/sample - loss: 2.0935 - val_loss: 2.3786\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope(): \n",
    "  seq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "  seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')\n",
    "  seq2seq_Model.summary() \n",
    "  script_name_base = 'tutorial_seq2seq'\n",
    "  csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "  model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                   save_best_only=True)\n",
    "\n",
    "  batch_size = 1200\n",
    "  epochs = 7\n",
    "  history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 950400 samples, validate on 129600 samples\n",
      "950400/950400 [==============================] - 68s 72us/sample - loss: 1.8455 - val_loss: 2.4658\n"
     ]
    }
   ],
   "source": [
    "seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.00001), loss='sparse_categorical_crossentropy')\n",
    "batch_size = 1200\n",
    "epochs = 1\n",
    "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "88p2g6UBzf3-"
   },
   "source": [
    "** Examine Model Architecture Summary **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZfrKiF9zf4J"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SR12Favqzf4U"
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "seq2seq_Model.save('seq2seq/seq2seq_model_tutorial.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "seq2seq_Model=load_model('seq2seq/seq2seq_model_tutorial.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ADEX-Hozf4e"
   },
   "source": [
    "# See Results On Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S_L60Jnzzf4f"
   },
   "outputs": [],
   "source": [
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
    "                                 decoder_preprocessor=title_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FVsJzITzzf4o",
    "outputId": "f71bbe6a-d5eb-4910-a188-3d8fec54438d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 65817 =================\n",
      "\n",
      "\"https://github.com/Cisco-Talos/snort-faq/issues/13\"\n",
      "Issue Body:\n",
      " i'm doing research trying to figure out how to add users using the -u along with snort but i can't find any documentation on how or where to create the rule to add a new userid \n",
      "\n",
      "Original Title:\n",
      " how to add a user to snort\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " how to add a new user\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 33225 =================\n",
      "\n",
      "\"https://github.com/autolab/Autolab/issues/916\"\n",
      "Issue Body:\n",
      " we should recommend text in annotations, since graders likely type the same responses over and over \n",
      "\n",
      "Original Title:\n",
      " hotkeys like gradescope\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " recommend annotations for a text box\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 4011 =================\n",
      "\n",
      "\"https://github.com/Virtual-Labs/molecular-florescence-spectroscopy-responsive-lab-iiith/issues/85\"\n",
      "Issue Body:\n",
      " defect description: the quiz page of effects of excimer and exciplex formations on fluorescence emission experiment of this lab, the alignment for the post experiment quiz is improper. need to fix the issue to maintain the look and feel of the page. actual result : the quiz page of effects of excimer and exciplex formations on fluorescence emission experiment of this lab, the alignment for the post experiment quiz is improper. environment : os: windows 7, ubuntu-16.04,centos-6 browsers: firefox-42.0,chrome-47.0,chromium-45.0 bandwidth : 100mbps hardware configuration:8gbram , processor:i5 test step link: https://github.com/virtual-labs/molecular-florescence-spectroscopy-responsive-lab-iiith/blob/master/test-cases/integration_test-cases/effects-of-excimer-and-exciplex-formations-of-fluorescence-emission/effects-of-excimer-and-exciplex-formations-of-fluorescence-emission_06_quiz_smk.org attachment: ! qa_mfs_i82 https://cloud.githubusercontent.com/assets/13479177/22054041/d7ba363e-dd79-11e6-8218-5f8636d2675d.png \n",
      "\n",
      "Original Title:\n",
      " qa_molecular fluorescence spectroscopy_expt-9_quiz_improper alignment\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " qa number quiz submission of number number number number number number\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 64248 =================\n",
      "\n",
      "\"https://github.com/xtacocorex/CHIP_IO/issues/61\"\n",
      "Issue Body:\n",
      " i spent quite some time figuring out why my code didn't use my polarity setting just to find this line: https://github.com/xtacocorex/chip_io/blob/master/source/c_pwm.c l531 is there still a reason for the chip-pro check? setting it manually using echo normal > /sys/class/pwm/pwmchip0/pwm0/polarity worked just fine. \n",
      "\n",
      "Original Title:\n",
      " polarity not set for normal chip\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " chip pro setting\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 49094 =================\n",
      "\n",
      "\"https://github.com/SirCmpwn/sway/issues/1157\"\n",
      "Issue Body:\n",
      " on a scaled output, you're able to move the cursor past the left and top edges of the screen. how far it goes depends on the scale. also, if you go near the bottom or right side of the screen, it cursor will stop short before it reaches it, and disappears. it seems that the mouse bounds and the output bounds don't line up properly. also, it doesn't seem to be sending the mouse events to the correct clients. if you click on swaybar near the bottom, it will click the window open below it instead. that also seems to suggest that the mouse and the output aren't lined up properly. \n",
      "\n",
      "Original Title:\n",
      " mouse pointer can go past edge of screen on scaled outputs\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " mouse bounds on the edges\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 91849 =================\n",
      "\n",
      "\"https://github.com/aspnet/Localization/issues/343\"\n",
      "Issue Body:\n",
      " right now if your full class name starts with the name of the assembly it will be trimmed. so for a project with assembly project.name.dll the class project.name.startup would have the resource startup.resx . this seems to cause a lot of confusion and trouble 318, 314 and many more . perviously the documentation in this area was incorrect, which likely lead to a lot of this confusion, but problems still persist and this is the most common type of issues filed on localization. we've already added searchedlocation to localizerstring to help with this, but people still seem to be having trouble so here are some of the options i've come up with. 1. create a mode on resourcemanagerstringlocalizer which will throw an exception with a detailed error message about where it looked when a resource isn't found instead of returning the key as a result. the idea here is that it makes it clearer when a resource isn't found rather than hiding that behind the key as default . 2. don't do any trimming of class names. so the resource for project.name.startup would always be project.name.startup.resx no matter what the assemblyname is. we could roll this out slowly by allowing fallback to the trimmed name for some time or forever . the idea here is to reduce the complexity of figuring out what to name resource to the simplest possible thing, even if it means having longer resource names or more deeply nested resource folders . \n",
      "\n",
      "Original Title:\n",
      " reconsider resource naming rules\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " assembly naming convention\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 60545 =================\n",
      "\n",
      "\"https://github.com/magepal/magento2-gmailsmtpapp/issues/87\"\n",
      "Issue Body:\n",
      " hi, in order to be able to update magento to the latest 2.2.1 release i had to do some tweaks as suggested in this https://magento.stackexchange.com/a/197371/48717 answer. after this, the installation process went through, but the magepal smtp plugin stopped working, i have now been trying to uninstall and reinstall using composer. but when i try to run the php -f bin/magento module:enable --clear-static-content magepal_gmailsmtpapp command to reinstall the plugin i get the following error: > autoload error: module 'magepal_gmailsmtpapp' from '/opt/bitnami/apps/magento/htdocs/app/code/magepal/gmailsmtpapp' has been already defined in '/opt/bitnami/apps/magento/htdocs/vendor/magepal/magento2-gmailsmtpapp' and i get stuck here not knowing what to do in order to sort this out, it seems to me that i will have to remove some remains of the previous plugin before trying to reinstall but i am not sure how to go about this. \n",
      "\n",
      "Original Title:\n",
      " upgrade to magento 2.2.1 requires plugin to be removed and causes reinstallation issues.\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " magento number 1 smtp error\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 77152 =================\n",
      "\n",
      "\"https://github.com/cal2195/Sandbox/issues/5\"\n",
      "Issue Body:\n",
      " > awesome! it's annoying having to install all my software everytime though... can you help? > no problem! you need sand grains! these are little pre-configured scripts for automatically installing software in your sandboxes! simply use the -g flag, comma separating any sand grains you would like to install! hi, this sounds interesting, but i don't understand the necessity of the grains. e.g. if i already have firefox installed, why would i need the grain to install it again into the sandbox? i thought the whole point of unionfs was to make use of the underlying filesystem by copy-on-write. also, it would help if you could write a brief comparison with other sandboxing tools, cgroups, container tools, etc. it's hard to understand how this fits in and what use-cases it would be best for. : thanks. \n",
      "\n",
      "Original Title:\n",
      " question about grains\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " install script automatically\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 706 =================\n",
      "\n",
      "\"https://github.com/CanonicalLtd/serial-vault/issues/134\"\n",
      "Issue Body:\n",
      " the account assertions section was originally designed with a single tenant environment where there would be just one or two accounts. in a multi-tenant environment there can be many accounts - at least one per customer, so the display needs to be changed so it handles more accounts. same for the account-key assertions. \n",
      "\n",
      "Original Title:\n",
      " wishlist: redesign the accounts section\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " account for single account manager\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 50840 =================\n",
      "\n",
      "\"https://github.com/facebook/react/issues/10031\"\n",
      "Issue Body:\n",
      " bug report i rely on a saved ref from the <fabric> component to do some layout formatting for my excel add-in. i used the ref property of the <fabric> component, and save a reference to alias.refs.root . between 4.9.0 and 4.9.1, this functionality broke - alias.refs.root is no longer defined. react version: 15.4.2 up to 15.6.1 browser: reproduced on safari and chrome macos , and chrome windows here is a codepen reproducing the behavior: https://codepen.io/tcarruthers/pen/vzjeaw thanks! nick \n",
      "\n",
      "Original Title:\n",
      " refs.root breaks between 4.9.0 and 4.9.1\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " ref and ref are not being used for ref\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 28018 =================\n",
      "\n",
      "\"https://github.com/josephr5000/confdb-public/issues/399\"\n",
      "Issue Body:\n",
      " currently you need to be an admin to be able to manage the library, including: - upload files - create folders - edit/move files - delete files system should enable conference admins to add a librarian role to users, which would grant them operational control over the library. specifically, it should let them do the 4 actions listed above. \n",
      "\n",
      "Original Title:\n",
      " user story: members want to volunteer to manage doc library without being admins\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add admin role to manage users\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 107322 =================\n",
      "\n",
      "\"https://github.com/thexerteproject/xerteonlinetoolkits/issues/556\"\n",
      "Issue Body:\n",
      " just testing use of milestones so this isn't actually an issue. just testing in advance of tidying up labels and using milestones too if they work as expected \n",
      "\n",
      "Original Title:\n",
      " test 1 - just testing use of milestones so this isn't actually an issue\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " using the same name as a key in the header\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 119764 =================\n",
      "\n",
      "\"https://github.com/CodingTrain/Rainbow-Topics/issues/633\"\n",
      "Issue Body:\n",
      " i would like to see a / many tutorial s about shadow mapping in glsl using processing! ! shadow mapping http://www.opengl-tutorial.org/assets/images/tuto-16-shadow-mapping/softshadows_wide.png \n",
      "\n",
      "Original Title:\n",
      " glsl shader: shadow mapping\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " shadow of mapping tutorial\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 35945 =================\n",
      "\n",
      "\"https://github.com/RagtagOpen/nomad/issues/154\"\n",
      "Issue Body:\n",
      " /carpools/find shows capacity but does not update when a user requests a ride; should also show available seats there need to be something tracking the awaiting approval ride state to show how many seats are left \n",
      "\n",
      "Original Title:\n",
      " webui: /carpools/find does not show available seat left\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " show current capacity and not available for the user\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 49230 =================\n",
      "\n",
      "\"https://github.com/jeremyevans/sequel/issues/1383\"\n",
      "Issue Body:\n",
      " complete description of issue we should call subset or where inside teh dataset_module , not model class. _subset_conditions_ plugin still define methods on model class and not on dataset_module , causing name error. simplest possible self-contained example showing the bug require 'sequel' db = sequel.sqlite create an items table db.create_table :satans do primary_key :id integer :num end class satan < sequel::model plugin :subset_conditions dataset_module do subset :whole_satan, num: 666 subset :half_satan, num: 333 subset :satan, whole_satan_conditions | half_satan_conditions end end satan.create num: 666 satan.create num: 333 satan.create num: 111 puts satan.satan.count this works if you call teh last subset outside dataset_module or use satan.whole_satan_conditions and satan.half_satan_conditions . but not _as-is_ full backtrace of exception if any $ ruby a.rb a.rb:17:in block in <class:satan>': undefined local variable or method whole_satan_conditions' for < <class:0x0056441d0c1658>:0x0056441d0c1400> nameerror from /home/rogi/.gem/ruby/2.4.0/gems/sequel-4.47.0/lib/sequel/model/base.rb:357:in module_eval' from /home/rogi/.gem/ruby/2.4.0/gems/sequel-4.47.0/lib/sequel/model/base.rb:357:in dataset_module' from a.rb:14:in <class:satan>' from a.rb:11:in <main>' \n",
      "\n",
      "Original Title:\n",
      " subset_conditions does not work on dataset_module.\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " subset model for dataset should not be subset\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 5142 =================\n",
      "\n",
      "\"https://github.com/ionic-team/ionic-cli/issues/2597\"\n",
      "Issue Body:\n",
      " _from @arapocket on august 2, 2017 2:47_ ionic version: check one with x 1.x for ionic 1.x issues, please use https://github.com/ionic-team/ionic-v1 2.x x 3.x i'm submitting a ... check one with x x bug report feature request support request => please do not submit support requests here, use one of these channels: https://forum.ionicframework.com/ or http://ionicworldwide.herokuapp.com/ current behavior: - position gets watched when i do a fresh run of the app using ionic cordova run ios --device -l -c - when i make changes to my code in this case just saving app rebuilds but device does not ask me to allow location tracking, thus breaking the geolocation. other information: - device only asks for allowing location when i restart my device edit: restarting the device doesn't help either. deleting the app and rebuilding it works. edit 2: deleting the app doesn't even work now!!! edit 3: ran the command again and it worked, but now watchposition is not working properly. when installing the app without livereload everything works fine. - nothing happening in the console to indicate an error _copied from original issue: ionic-team/ionic 12542_ \n",
      "\n",
      "Original Title:\n",
      " livereload halts geolocation\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " bug ion datetime can t be set when using watch mode\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 9964 =================\n",
      "\n",
      "\"https://github.com/tensorflow/tensorflow/issues/9095\"\n",
      "Issue Body:\n",
      " @mrry hi, i am confused with a prediction problem implemented in tensorflow seq2seq. i found that in seq2seq inference code, there must be a embedding matrix to step forward. but now, my problem is a continuous prediction problem, such as price prediction . the output must be a sequence of floating numbers. i don't know how to embed these numbers , so what should i do with tensorflow seq2seq? thanks a lot!! \n",
      "\n",
      "Original Title:\n",
      " problem with seq2seq models in prediction\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " prediction for prediction\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 27986 =================\n",
      "\n",
      "\"https://github.com/modelica/Modelica/issues/1477\"\n",
      "Issue Body:\n",
      " reported by awittkopf on 8 may 2014 20:06 utc we recently attempted to simulate modelica.blocks.examples.booleannetwork1 and ran into a problem at initialization time. in the flattened model we have the following 2 initial equations:\n",
      "i1: pre xor1.u 2 = false\n",
      "i2: pre or2.u 1 = false and the following 4 system equations:\n",
      "e1: nand1.y = or2.u 1 e2: nand1.y = not and nand1.u 1 ,nand1.u 2 e3: and1.u 3 = nand1.u 2 e4: and1.u 3 = xor1.u 2 now unless we misunderstood the specification, presence of a 'pre' for a discrete variable adds the equation:\n",
      "pre var = var\n",
      "to the initialization system, so given this we should be able to work with the pre values to obtain a suitable initialization for the discrete variables. now e4 + i1 =>\n",
      "i3: pre and1.u 3 = false then e3 + i3 =>\n",
      "i4: pre nand1.u 2 = false also e1 + i2 =>\n",
      "i5: pre nand1.y = false then applying i4,i5 + e2 gives:\n",
      "i6: false = not and nand1.u 1 ,false => false = not false => false = true if out understanding is correct, there is a flaw in the initial equations for the model. if our understanding is incorrect, please let us know what has been interpreted incorrectly. ---- migrated-from: https://trac.modelica.org/modelica/ticket/1477 \n",
      "\n",
      "Original Title:\n",
      " issue with modelica.blocks.examples.booleannetwork1\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " modelica blocks examples on a small parameter\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 117920 =================\n",
      "\n",
      "\"https://github.com/codyborders/abandoned_camel_coffee_mercantile/issues/65\"\n",
      "Issue Body:\n",
      " as an authenticated admin, when i visit an individual order page then i can see the order's date and time. and i can see the purchaser's full name and address. and i can see, for each item on the order: - the item's name, which is linked to the item page. - quantity in this order. - price - line item subtotal. and i can see the total for the order. and i can see the status for the order. \n",
      "\n",
      "Original Title:\n",
      " admin views an individual folder\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " admin views order\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 7742 =================\n",
      "\n",
      "\"https://github.com/gskbyte/GSKStretchyHeaderView/issues/55\"\n",
      "Issue Body:\n",
      " hi team, can you make an example with scalable view/image in place of scalable text. \n",
      "\n",
      "Original Title:\n",
      " scalable image instead of scaleable text\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " example with image\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 40941 =================\n",
      "\n",
      "\"https://github.com/KnpLabs/KnpMenuBundle/issues/372\"\n",
      "Issue Body:\n",
      " i'm trying out symfony 3.4 with flex as a prelude to updating my symfony 3.3 application. after installing knpmenubundle and creating a builder service i attempted to render a menu. this is the error i received: > unable to find template knp_menu.html.twig looked into: /test/ci-flex/templates, /test/ci-flex/templates, /test/ci-flex/vendor/symfony/twig-bridge/resources/views/form in @knpmenu/menu.html.twig at line 1. the initial symfony installation was done like this: composer create-project symfony/skeleton:3.4 ci-flex . below is part of my composer file so you can see what components of symfony i've installed. do i need another component so that knpmenubundle will work? or is the error something that should be addressed inside knpmenubundle? require : { php : ^7.0.8 , friendsofsymfony/jsrouting-bundle : ^2.0 , knplabs/knp-menu-bundle : ^2.2 , sensio/framework-extra-bundle : ^5.1 , stof/doctrine-extensions-bundle : ^1.2 , symfony/asset : ^3.4 , symfony/console : ^3.4 , symfony/flex : ^1.0 , symfony/framework-bundle : ^3.4 , symfony/lts : ^3 , symfony/orm-pack : ^1.0 , symfony/security-bundle : ^3.4 , symfony/swiftmailer-bundle : ^2.3 , symfony/translation : ^3.4 , symfony/twig-bundle : ^3.4 , symfony/validator : ^3.4 , symfony/web-server-bundle : ^3.4 , symfony/yaml : ^3.4 , }, require-dev : { symfony/dotenv : ^3.4 , symfony/profiler-pack : ^1.0 } \n",
      "\n",
      "Original Title:\n",
      " {% extends 'knp_menu.html.twig' %} throws error in symfony 3.4.1 with flex\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " twig error when using twig template\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 4742 =================\n",
      "\n",
      "\"https://github.com/shanesmith/bash-sneak/issues/2\"\n",
      "Issue Body:\n",
      " it just doesn't do anything... plus it seems to mess up the executed command... \n",
      "\n",
      "Original Title:\n",
      " cannot quit sneak prompt with ctrl-c\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " the first time you should be able to execute the command\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 40483 =================\n",
      "\n",
      "\"https://github.com/whosonfirst-data/whosonfirst-data/issues/766\"\n",
      "Issue Body:\n",
      " the us census publishes annual changes to places in the united states. take a look at these changes and update who's on first records accordingly. these changes are updated through 2015 and, at first glance, it looks like who's on first still has a few outdated names and geometries. for example: the former helena and mcrae cities in georgia have merged to form the new ‘incorporated place entity’, mcrae-helena city. this new incorporated place has a fips code of 49100 and an ansi code of 02770965. we still maintain records for helena https://whosonfirst.mapzen.com/spelunker/id/85936629/ 13/32.0796/-82.9054 and mcrae https://whosonfirst.mapzen.com/spelunker/id/85936919/ 13/32.0545/-82.8857 . https://www.census.gov/programs-surveys/acs/technical-documentation/table-and-geography-changes.html \n",
      "\n",
      "Original Title:\n",
      " review changes from us census table and geography changes page\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " update the publishing to the new paper\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 114557 =================\n",
      "\n",
      "\"https://github.com/rhalbersma/xstd/issues/2\"\n",
      "Issue Body:\n",
      " int_set unit test fails on travis ci, but only if cmake is installed from the .sh instead of the .tar.gz archive ~~~ 3: /home/travis/build/rhalbersma/xstd/test/src/int_set.cpp 281 : error in backwarditerationtraversesrange<n4xstd7int_setili50eee> : check it == rend b .base failed 3: /home/travis/build/rhalbersma/xstd/test/src/int_set.cpp 293 : error in reverseforwarditerationtraversesrange<n4xstd7int_setili50eee> : check it.base == begin b failed 3: 3: 2 failures detected in test suite master test suite 3/3 test 3: test.int_set ..................... failed 0.17 sec ~~~ \n",
      "\n",
      "Original Title:\n",
      " investigate int_set iterator bug\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " travis ci fails on travis\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 116046 =================\n",
      "\n",
      "\"https://github.com/DeaDBeeF-Player/deadbeef/issues/1904\"\n",
      "Issue Body:\n",
      " ! deepinscreenshot_20171003111616 https://user-images.githubusercontent.com/20014762/31108574-be9d3272-a82c-11e7-9ae5-76ec30c0cef2.png steps to reproduce the problem scan selection as albums by tags , different replaygain for the songs from the same album and same disc. what's going on? describe the problem in as much detail as possible. there are 6 discs in the album, when i scan them as albums by tags , those songs in the same disc and same album have different replaygain. information about the software: deadbeef version: deadbeef-git r7416.6d3fc012-1 os: angergos \n",
      "\n",
      "Original Title:\n",
      " wrong album gain form scan selection as albums by tags\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " different album selection in the same time\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 118240 =================\n",
      "\n",
      "\"https://github.com/phw198/OutlookGoogleCalendarSync/issues/390\"\n",
      "Issue Body:\n",
      " ogcs version : v2.6.0.0 installed or portable : installed problem description i have a simple setup of one local outlook calendar syncing to my central google calendar. i create a lot of appointments in outlook by copying a previous appointment and pasting say, to a few weeks later than the original appointment . when i do so, on the next sync, these outlook appointments created through the copy-paste are not replicated into the google calendar. then on the next sync after that, the outlook calendar appointments created through the copy-paste are deleted! note that i have no opportunity to use the “recurring” appointment feature of outlook as there is not sufficient regularity to the appointments. also note i use copy-paste rather than re-type new appointments each time because it is much faster and less cumbersome, given there is quite a bit of info in each appointment steps to reproduce the issue set up a sync from a local outlook to a central google in outlook, take an existing appointment, copy it with cntrl-c, go to the next week, replicate it with cntrl-v, edit the new entry just to make sure it looks right, or make some changes doesn’t affect the problem , save it. make a sync happen. make another sync happen. voila – pasted appointment in outlook is gone any other information . \n",
      "\n",
      "Original Title:\n",
      " fails to sync properly when outlook appointments created via copy/paste\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " sync over the same content in outlook calendar\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 115382 =================\n",
      "\n",
      "\"https://github.com/aspnet/EntityFramework/issues/8673\"\n",
      "Issue Body:\n",
      " also, look at whether the appropriate object is passed down to the payload, rather than just string. check these types, and any others that are query-related: - binaryexpressioneventdata - includeeventdata - navigationpatheventdata - querymodelexpressioneventdata \n",
      "\n",
      "Original Title:\n",
      " api review: review/rename query eventdata classes\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " check that the current file is passed to the server\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 18964 =================\n",
      "\n",
      "\"https://github.com/Eole-dev/authphpbb3/issues/7\"\n",
      "Issue Body:\n",
      " when logging in to or out of the wiki it will redirect me to www.site-name.com my wordpress instead of forum.site-name.com my phpbb3 is this something set in phpbb3? can i change this to redirect users properly? aside from that, logging in to my forum does log me in to my wiki \n",
      "\n",
      "Original Title:\n",
      " change login redirect\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " redirect to url\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 58045 =================\n",
      "\n",
      "\"https://github.com/getgrav/grav/issues/1728\"\n",
      "Issue Body:\n",
      " i tried to apply the same logic to my theme's configuration file as well but it does not seem to work. i created the following directory localhost/themes/mytheme/mytheme.yaml . i know the documentation https://learn.getgrav.org/advanced/environment-config only mentions system.yaml site.yaml and plugin configurations , but i was wondering if it was in the plans to also support this feature for themes ? my use case being that i have a config variable called env where possible values are development|production that defines how twig should render templates depending on the value of said config variable. on localhost i would have development on localhost and production on the server. as a workaround i defined the variable in site instead of the teme's config file. \n",
      "\n",
      "Original Title:\n",
      " automatic environment configuration theme config\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " does not work with theme configuration\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 119508 =================\n",
      "\n",
      "\"https://github.com/mozilla-mobile/focus-android/issues/122\"\n",
      "Issue Body:\n",
      " similar to what fennec does: we could potentially implement our own locale switching, to allow overriding the os locale. see fennec bugs: https://bugzilla.mozilla.org/show_bug.cgi?id=936756 https://bugzilla.mozilla.org/show_bug.cgi?id=917480 this seems quite complicated, so i'm guessing we won't want this in v1 - but it might be nice to have in future. \n",
      "\n",
      "Original Title:\n",
      " implement focus-internal locale switching\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " locale switching to locale\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 99927 =================\n",
      "\n",
      "\"https://github.com/apex/log/issues/40\"\n",
      "Issue Body:\n",
      " > this might be a dumb idea, so feel free to kick me out :p --- i think it will be very handy to check if a given error implements fielder in witherror and add its .fields to the returned context . golang package main import github.com/apex/log type validationerror struct { message string // holds a general validation error errors map string string // a map of field => errors } func ve validationerror error string { return ve.message } func ve validationerror fields log.fields { ret := make log.fields, len ve.errors for field, errs := range ve.errors { ret field = errs } return ret } func validate error { return &validationerror{ message: validation error , errors: map string string{ type : string{ type must be one of foo/baz/woot , type is required , }, }, } } func main { err := validate if err != nil { log.witherror err .fatal validation error } } so output with a fielder check would be: golang ~ go run /tmp/test.go 2017/04/11 23:33:46 fatal validation error error=validation error type= type must be one of foo/baz/woot type is required exit status 1 \n",
      "\n",
      "Original Title:\n",
      " errors implementing fielder?\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add support for fields in the package\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 19418 =================\n",
      "\n",
      "\"https://github.com/koorellasuresh/UKRegionTest/issues/2903\"\n",
      "Issue Body:\n",
      " first from flow in uk south \n",
      "\n",
      "Original Title:\n",
      " first from flow in uk south\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " first from flow in uk south\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1135 =================\n",
      "\n",
      "\"https://github.com/loggingroads/onboarding/issues/66\"\n",
      "Issue Body:\n",
      " all pages are called onboarding \n",
      "\n",
      "Original Title:\n",
      " use correct page + site name for page tiles\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " remove all pages\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 61729 =================\n",
      "\n",
      "\"https://github.com/xgfe/react-native-datepicker/issues/145\"\n",
      "Issue Body:\n",
      " i followed all the steps as mentioned in the readme. only date with icon is displaying not picker. when user clicks on date-icon/date, datepicker is not showing up. \n",
      "\n",
      "Original Title:\n",
      " date picker not showing up\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " not working with date picker\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 61677 =================\n",
      "\n",
      "\"https://github.com/lstjsuperman/fabric/issues/10284\"\n",
      "Issue Body:\n",
      " in android.app.activitythread.collectcomponentcallbacks number of crashes: 1 impacted devices: 1 there's a lot more information about this crash on crashlytics.com: https://fabric.io/momo6/android/apps/com.immomo.momo/issues/599d410fbe077a4dcc7bda53?utm_medium=service_hooks-github&utm_source=issue_impact https://fabric.io/momo6/android/apps/com.immomo.momo/issues/599d410fbe077a4dcc7bda53?utm_medium=service_hooks-github&utm_source=issue_impact \n",
      "\n",
      "Original Title:\n",
      " activitythread.java line 4546\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " activitythread java line number\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 63282 =================\n",
      "\n",
      "\"https://github.com/mengxiong10/vue2-datepicker/issues/2\"\n",
      "Issue Body:\n",
      " i am using vue.js 2 and got the following error: ... error in ./~/vue2-datepicker/src/datepicker/index.vue module build failed: error: couldn't find preset latest relative to directory .../node_modules/vue2-datepicker ... bug reproduction can be found here https://github.com/blackpuppy/vue2-datepicker-demo . \n",
      "\n",
      "Original Title:\n",
      " error: couldn't find preset latest\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " error couldn t find preset es2015 relative to directory\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 23653 =================\n",
      "\n",
      "\"https://github.com/fnoop/vision_landing/issues/12\"\n",
      "Issue Body:\n",
      " detection is very poor under low light or other conditions. add an option to alter gain/brightness, also detection thresholds. \n",
      "\n",
      "Original Title:\n",
      " add gain/brightness option\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " low level detection\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 26925 =================\n",
      "\n",
      "\"https://github.com/pilosa/pilosa/issues/722\"\n",
      "Issue Body:\n",
      " description pilosa needs a fairly high open file limit to operate in many cases. this is a common sticking point for new users, and can also be an annoyance for experienced users. go's syscall package provides some os dependent functions for setting the open file limit from within the process - a small example that seems to work on osx is: https://play.golang.org/p/j5ni_iihcw the meat of this is lim := &syscall.rlimit{} err := syscall.getrlimit syscall.rlimit_nofile, lim err = syscall.setrlimit syscall.rlimit_nofile, &syscall.rlimit{cur: lim.max, max: lim.max} we should try implementing this, and make sure it works on darwin and linux. we could try compiling for bsd and other systems as well, and if it doesn't work, we can use file suffixes i.e. file_darwin.go to exclude the functionality for those platforms. i think that will work, but we'll need to confirm the logic here will be something like: 1. check current limit - if unable to check, log a message notifying the users. if limit is high enough 200k? we're done. otherwise... 2. try to set the limit higher we've been using 262144 - alternatively use lim.max as in the example. if there's an error, log out that the limit may not be high enough, and what the error was setting it. 3. check to see what the limit was set to. if high enough, good, otherwise log out again that we weren't able to set it high enough. success criteria what criteria will consider this ticket closeable? pilosa tries its best to set the open file limit high enough, logs clear messages describing the situation if it isn't able to, and gives the user something searchable to get further instructions for setting open file limits. \n",
      "\n",
      "Original Title:\n",
      " investigate and implement use of syscall.setrlimit to change open file limit.\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add a high cpu usage for this package\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 71547 =================\n",
      "\n",
      "\"https://github.com/18F/laptop/issues/152\"\n",
      "Issue Body:\n",
      " this repository has been using travis ci, however access to travis ci is now turning off. when you need to use this repository again, convert it to a ci/cd system which is in the itsp such as circle ci. \n",
      "\n",
      "Original Title:\n",
      " move to a new ci/cd system\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " convert to a continuous integration\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 41804 =================\n",
      "\n",
      "\"https://github.com/squirrel-project/squirrel_robotino/issues/216\"\n",
      "Issue Body:\n",
      " i am not sure if i am doing something wrong, as i don't know much abut urdf, but running the bringup for tuw_robotino2.launch with the current version of this repo, i get: ... logging to /u/vienna/.ros/log/c587a078-b429-11e7-a2aa-00012e79a4f8/roslaunch-scrat-9659.log checking log directory for disk usage. this may take awhile. press ctrl-c to interrupt done checking log file disk usage. usage is <1gb. traceback most recent call last : file /opt/ros/indigo/share/xacro/xacro.py , line 62, in <module> xacro.main file /opt/ros/indigo/lib/python2.7/dist-packages/xacro/__init__.py , line 696, in main eval_self_contained doc file /opt/ros/indigo/lib/python2.7/dist-packages/xacro/__init__.py , line 626, in eval_self_contained eval_all doc.documentelement, macros, symbols file /opt/ros/indigo/lib/python2.7/dist-packages/xacro/__init__.py , line 526, in eval_all str name , str node.tagname xacro.xacroexception: invalid parameter base_neck_pitch while expanding macro xacro:shell while processing /u/vienna/catkin_ws/src/squirrel_robotino/robotino_bringup/robots/tuw-robotino2/launch/tuw-robotino2.launch: invalid <param> tag: cannot load command parameter robot_description : command /opt/ros/indigo/share/xacro/xacro.py '/u/vienna/catkin_ws/src/squirrel_robotino/robotino_bringup/robots/tuw-robotino2/urdf/robotin o.urdf.xacro' returned with code 1 . param xml is <param command= $ find xacro /xacro.py '$ find robotino_bringup /robots/$ arg robot /urdf/robotino.urdf.xacro' name= robot_description /> the traceback for the exception was written to the log file what is up with the base_neck_pitch ? the last version of the bringup that worked didn't have as many link defined in the properties.urdf.xacro, notably base_neck was not in there yet. \n",
      "\n",
      "Original Title:\n",
      " robotino_bringup with the new urdf\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " can t log in\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1955 =================\n",
      "\n",
      "\"https://github.com/Rotonde/rotonde-client/issues/178\"\n",
      "Issue Body:\n",
      " i would like for us to find a solution on how we could share audio and video content from p2pn sites, onto rotonde and elsewhere. this can be planned for post-0.8 beaker release. \n",
      "\n",
      "Original Title:\n",
      " p2p-native solution for safe content embed\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " how to share audio and video content on sites\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 26099 =================\n",
      "\n",
      "\"https://github.com/fifafu/BetterTouchTool/issues/917\"\n",
      "Issue Body:\n",
      " description of bug/feature request/question: it would be very handy to be able to print a screenshot directly from the window that opens after you do a screen capture using btt. right now i have to copy to clipboard and then paste into word and then print from there. i do a lot of this. affected input device e.g. macbook trackpad, magic mouse/trackpad, touch bar, etc. : device information: type of mac: macos version: bettertouchtool version: additional information e.g. stacktraces, related issues, screenshots, workarounds, etc. : --- note: before bug reporting, please make sure you have the latest version of bettertouchtool and that you have already tried to restart your system :- . if you encounter a crash, please attach a crash log from the macos console.app from the user diagnostic reports section. \n",
      "\n",
      "Original Title:\n",
      " feature request: print directly from screen capture window\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " print screenshot to clipboard\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 117199 =================\n",
      "\n",
      "\"https://github.com/Planktom/pending_task_rescaler/issues/4\"\n",
      "Issue Body:\n",
      " add something like a high prio feature. will be read from maybe tasks labels \n",
      "\n",
      "Original Title:\n",
      " add important option\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " high priority high scores\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 20894 =================\n",
      "\n",
      "\"https://github.com/frictionlessdata/ckanext-validation/issues/10\"\n",
      "Issue Body:\n",
      " providing an existing schema that has been created elsewhere, probably by hand, is a great feature but one that will be probably only used by power users or data-savvy publishers that are familiar with validation, standards, etc. to really engage publishers in the description of their data we need to guide the creation of these schemas based on the actual contents of the file, what people is familiar with. in a nutshell, when uploading or linking to a new file, the user gets a list of the existing fields, with an option to define the type of that field a guessed one is provided for them . additionally they can provide extra information about the field like user-friendly labels or a description. this gets transformed into a table schema internally that gets stored in the schema field. this pattern is well established see eg socrata https://support.socrata.com/hc/en-us/articles/202950128 , the challenge is how to integrate it in the existing workflow in ckan for creating a dataset. if the user uploads the file it seems like the best experience would be to read the file in the browser using tableschema-js https://github.com/frictionlessdata/tableschema-js and infer a schema that would be used to generate the edit interface. this tool shows how the general interaction would be and perhaps we can reuse part of it : https://csv-schema.surge.sh/ estimate: 4-6 days needs proper spec \n",
      "\n",
      "Original Title:\n",
      " schema editor based on the actual fields\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " create a schema based on the schema of the new object\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 104222 =================\n",
      "\n",
      "\"https://github.com/clarat-org/clarat/issues/1089\"\n",
      "Issue Body:\n",
      " daten werden beim editieren nicht korrekt geladen bzw. nicht korrekt angezeigt? beim speichern wird dann anscheinend sogar ein neues modell erstellt, was auch ein grobes fehlverhalten ist. bitte einarbeiten dürfte die genericform sein und auf fehlersuche gehen. ist an sich ein sehr kritischer bug, der aber gerade de facto wenige user betrifft - nichtsdestotrotz muss er natürlich behoben werden. \n",
      "\n",
      "Original Title:\n",
      " bug: nutzer & nutzerteam im backend 2.0 funktioniert nicht\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " login und nicht\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 117491 =================\n",
      "\n",
      "\"https://github.com/matc4/react-native-svg-uri/issues/72\"\n",
      "Issue Body:\n",
      " i was reading the source code and see that the svg element is returned as an <svg> meaning that it can not run inside a <svg> element from react-native-svg, would it be possible to make it compatible with <svg> elements and render the svg uri inside them? add first i thought just removing the view element would solve the problem but you can not render nested svg elements sadly \n",
      "\n",
      "Original Title:\n",
      " render inside svg\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " svg element in svg\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 36684 =================\n",
      "\n",
      "\"https://github.com/stchristian/mealteam/issues/3\"\n",
      "Issue Body:\n",
      " there are a few rules in styles.css that use pixel instead of em or rem. for better working responsiveness i suggest using em / rem. \n",
      "\n",
      "Original Title:\n",
      " use of responsive units\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " use css styles instead of em\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 41610 =================\n",
      "\n",
      "\"https://github.com/curationexperts/ansible-hydra/issues/230\"\n",
      "Issue Body:\n",
      " allow configuration of syslog rotations from the system_setup role. current client requirement to keep 60 daily syslogs - we could make the number retained a variable. \n",
      "\n",
      "Original Title:\n",
      " add syslog rotation options\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add configuration flag to setup system wide\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 70803 =================\n",
      "\n",
      "\"https://github.com/ksAutotests/CreateValidAndUpdateInvalidTest/issues/351\"\n",
      "Issue Body:\n",
      " tutorial issue found: https://github.com/ksautotests/createvalidandupdateinvalidtest/blob/master/tutorials/firefox/tutorial_firefox.md https://github.com/ksautotests/createvalidandupdateinvalidtest/blob/master/tutorials/firefox/tutorial_firefox.md contains only invalid tags. your tutorial in sapcom was not updated. the invalid tags listed below. please double-check the following tags:\n",
      "- 12345\n",
      "- qwqwqw affected server: qa green \n",
      "\n",
      "Original Title:\n",
      " tutorial page tutorial_firefox.md issue. qa green\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " tutorial page tutorial firefox md issue qa green\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 51661 =================\n",
      "\n",
      "\"https://github.com/linuxdeepin/dde-control-center/issues/84\"\n",
      "Issue Body:\n",
      " the display scaling widget works well for scaling every display together, and the display manager works well for arranging the outputs, but putting one ui scaling value on multiple displays can make uis too small or large on some displays. could per-display scaling be added to the display manager? xrandr seems to support this currently. for example, when i plugged my 1080p monitors into my 4k display laptop, i used the manager to arrange the displays and make a profile - this works brilliantly - but to use the external displays i have to set the scaling to 1.0, which makes non-deepin windows like chrome too small on the laptop display. thanks for your work; i really enjoy using deepin at work and at home! \n",
      "\n",
      "Original Title:\n",
      " feature request: per-display scaling\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " display scaling scale\n"
     ]
    }
   ],
   "source": [
    "# this method displays the predictions on random rows of the holdout set\n",
    "seq2seq_inf.demo_model_predictions(n=50, issue_df=testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5FXEdqdvzf4x"
   },
   "source": [
    "# Feature Extraction Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45YfioPzzf4y"
   },
   "outputs": [],
   "source": [
    "with strategy.scope(): \n",
    "    # Read All 5M data points\n",
    "    all_data_df = pd.read_csv(INPUT_FILE).sample(n=1200000)\n",
    "    # Extract the bodies from this dataframe\n",
    "    all_data_bodies = all_data_df['body'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "po-Ms-uZzf4-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:...tokenizing data\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope(): \n",
    "    # transform all of the data using the ktext processor\n",
    "    all_data_vectorized = body_pp.transform_parallel(all_data_bodies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-0DR1Uczf5H"
   },
   "outputs": [],
   "source": [
    "# save transformed data\n",
    "with open('all_data_vectorized.dpkl', 'wb') as f:\n",
    "    dpickle.dump(all_data_vectorized, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ilHxGezzf5R"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "seq2seq_inf_rec = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
    "                                    decoder_preprocessor=title_pp,\n",
    "                                    seq2seq_model=seq2seq_Model)\n",
    "recsys_annoyobj = seq2seq_inf_rec.prepare_recommender(all_data_vectorized, all_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_GVQqxkzf5Y"
   },
   "source": [
    "### Example 1: Issues Installing Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJS9FWYvzf5Z",
    "outputId": "33023992-5e79-49e4-91c0-30495a287b75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 13563 =================\n",
      "\n",
      "\"https://github.com/bnosac/pattern.nlp/issues/5\"\n",
      "Issue Body:\n",
      " thanks for your package, i can't wait to use it. unfortunately i have issues with the installation. prerequisite is 'first install python version 2.5+ not version 3 '. so this package cant be used with version 3.6 64bit that i have installed? i nevertheless tried to install it using pip, conda is not supported? but got an error: 'syntaxerror: missing parentheses in call to 'print''. besides when i try to run the library in r version 3.3.3. 64 bit i got errors with can_find_python_cmd required_modules = pattern.db : 'error in find_python_cmd......' pattern seems to be written in python but must be used in r, why cant it be used in python? i found another python pattern application that apparently does the same in python: https://pypi.python.org/pypi/pattern how is this related? \n",
      "\n",
      "Original Title:\n",
      " error installation python\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " install with python * number *\n",
      "\n",
      "**** Similar Issues (using encoder embedding) ****:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_url</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>body</th>\n",
       "      <th>dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286906</th>\n",
       "      <td>\"https://github.com/scikit-hep/root_numpy/issues/337\"</td>\n",
       "      <td>root 6.10/02 and root_numpy compatibility</td>\n",
       "      <td>i am trying to pip install root_pandas and one of the dependency is root_numpy however some weird reasons i am unable to install it even though i can import root in python. i am working on python3.6 as i am more comfortable with it. is root_numpy is not yet compatible with the latest root?</td>\n",
       "      <td>0.694671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314005</th>\n",
       "      <td>\"https://github.com/andim/noisyopt/issues/4\"</td>\n",
       "      <td>joss review: installing dependencies via pip</td>\n",
       "      <td>hi, i'm trying to install noisyopt in a clean conda environment running python 3.5. running pip install noisyopt does not install the dependencies numpy, scipy . i see that you do include a requires keyword argument in your setup.py file, does this need to be install_requires ? as in https://packaging.python.org/requirements/ . also, not necessary if you don't want to, but i think it would be good to include a list of dependences somewhere in the readme.</td>\n",
       "      <td>0.698265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48120</th>\n",
       "      <td>\"https://github.com/turi-code/SFrame/issues/389\"</td>\n",
       "      <td>python 3.6 compatible</td>\n",
       "      <td>hi: i tried to install sframe using pip and conda but i can not find anything that will work with python 3.6? has sframe been updated to work with python 3.6 yet? thanks, drew</td>\n",
       "      <td>0.718715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    issue_url  \\\n",
       "286906  \"https://github.com/scikit-hep/root_numpy/issues/337\"   \n",
       "314005           \"https://github.com/andim/noisyopt/issues/4\"   \n",
       "48120        \"https://github.com/turi-code/SFrame/issues/389\"   \n",
       "\n",
       "                                         issue_title  \\\n",
       "286906     root 6.10/02 and root_numpy compatibility   \n",
       "314005  joss review: installing dependencies via pip   \n",
       "48120                          python 3.6 compatible   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                              body  \\\n",
       "286906                                                                                                                                                                          i am trying to pip install root_pandas and one of the dependency is root_numpy however some weird reasons i am unable to install it even though i can import root in python. i am working on python3.6 as i am more comfortable with it. is root_numpy is not yet compatible with the latest root?   \n",
       "314005  hi, i'm trying to install noisyopt in a clean conda environment running python 3.5. running pip install noisyopt does not install the dependencies numpy, scipy . i see that you do include a requires keyword argument in your setup.py file, does this need to be install_requires ? as in https://packaging.python.org/requirements/ . also, not necessary if you don't want to, but i think it would be good to include a list of dependences somewhere in the readme.   \n",
       "48120                                                                                                                                                                                                                                                                                              hi: i tried to install sframe using pip and conda but i can not find anything that will work with python 3.6? has sframe been updated to work with python 3.6 yet? thanks, drew   \n",
       "\n",
       "            dist  \n",
       "286906  0.694671  \n",
       "314005  0.698265  \n",
       "48120   0.718715  "
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq2seq_inf_rec.demo_model_predictions(n=1, issue_df=testdf, threshold=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FqNyBuBpzf5g"
   },
   "source": [
    "### Example 2:  Issues asking for feature improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oa5_nwm3zf5h",
    "outputId": "2d251bf5-03cb-4193-cc06-9fd16908fd9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 157322 =================\n",
      "\n",
      "\"https://github.com/Chingu-cohorts/devgaido/issues/89\"\n",
      "Issue Body:\n",
      " right now, your profile link is https://devgaido.com/profile. this is fine, but it would be really cool if there was a way to share your profile with other people. on my portfolio, i have social media buttons to freecodecamp, github, ect. without a custom link, i cannot show-off what i have done on devgaido to future employers. \n",
      "\n",
      "Original Title:\n",
      " feature request: sharable profile.\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add a link to your profile\n",
      "\n",
      "**** Similar Issues (using encoder embedding) ****:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_url</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>body</th>\n",
       "      <th>dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>250423</th>\n",
       "      <td>\"https://github.com/ParabolInc/action/issues/1379\"</td>\n",
       "      <td>integrations list view discoverability</td>\n",
       "      <td>issue - enhancement i was initially confused by the link to my account copy; seeing github in the integrations list made me think it had already been set up . i realize now that i had to allow parabol to post as me. i think that link to my account could use a tooltip explaining what link means, and why you'd want to do so. &lt;img width= 728 alt= screen shot 2017-09-29 at 10 52 05 am src= https://user-images.githubusercontent.com/2146312/31024786-2fd39c46-a50e-11e7-9f2a-6d4a5ed2baeb.png &gt;</td>\n",
       "      <td>0.748828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222304</th>\n",
       "      <td>\"https://github.com/viosey/hexo-theme-material/issues/166\"</td>\n",
       "      <td>allow us to use sns-share for github</td>\n",
       "      <td>i'd love to be able to add a link at the bottom of the page for my github account. however, the sns-share option doesn't currently seem to be able to do this.</td>\n",
       "      <td>0.774398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153327</th>\n",
       "      <td>\"https://github.com/tobykurien/GoogleApps/issues/31\"</td>\n",
       "      <td>drive provide download ability</td>\n",
       "      <td>sometimes people share files via g drive. provided a link this app can show some info about the files but doesn't show the download button. i hope that it can be fixed and users would be able to download files with this app.</td>\n",
       "      <td>0.778953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         issue_url  \\\n",
       "250423          \"https://github.com/ParabolInc/action/issues/1379\"   \n",
       "222304  \"https://github.com/viosey/hexo-theme-material/issues/166\"   \n",
       "153327        \"https://github.com/tobykurien/GoogleApps/issues/31\"   \n",
       "\n",
       "                                   issue_title  \\\n",
       "250423  integrations list view discoverability   \n",
       "222304    allow us to use sns-share for github   \n",
       "153327          drive provide download ability   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              body  \\\n",
       "250423  issue - enhancement i was initially confused by the link to my account copy; seeing github in the integrations list made me think it had already been set up . i realize now that i had to allow parabol to post as me. i think that link to my account could use a tooltip explaining what link means, and why you'd want to do so. <img width= 728 alt= screen shot 2017-09-29 at 10 52 05 am src= https://user-images.githubusercontent.com/2146312/31024786-2fd39c46-a50e-11e7-9f2a-6d4a5ed2baeb.png >   \n",
       "222304                                                                                                                                                                                                                                                                                                                                              i'd love to be able to add a link at the bottom of the page for my github account. however, the sns-share option doesn't currently seem to be able to do this.   \n",
       "153327                                                                                                                                                                                                                                                                            sometimes people share files via g drive. provided a link this app can show some info about the files but doesn't show the download button. i hope that it can be fixed and users would be able to download files with this app.   \n",
       "\n",
       "            dist  \n",
       "250423  0.748828  \n",
       "222304  0.774398  \n",
       "153327  0.778953  "
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq2seq_inf_rec.demo_model_predictions(n=1, issue_df=testdf, threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OxGtR_Izzf5q",
    "outputId": "d0d2fdfb-af53-417c-bbd5-de66a7bf3a40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# incase you need to reset the rec system\n",
    "# seq2seq_inf_rec.set_recsys_annoyobj(recsys_annoyobj)\n",
    "# seq2seq_inf_rec.set_recsys_data(all_data_df)\n",
    "\n",
    "# save object\n",
    "recsys_annoyobj.save('recsys_annoyobj.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SfZbr440zf54"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {
    "height": "263px",
    "width": "352px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
